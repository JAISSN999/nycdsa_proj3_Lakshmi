{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('F:/notebook_working/kaggle_compe/train.csv').fillna('No data')\n",
    "test = pd.read_csv('F:/notebook_working/kaggle_compe/test.csv').fillna('No data')\n",
    "\n",
    "#train.id = train.id.astype('str')\n",
    "#train.comment_text = train.comment_text.astype('str')\n",
    "\n",
    "#test.id = test.id.astype('str')\n",
    "#test.comment_text = test.comment_text.astype('str')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571,)\n",
      "(153164,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      "id               159571 non-null object\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 8.5+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153164 entries, 0 to 153163\n",
      "Data columns (total 2 columns):\n",
      "id              153164 non-null object\n",
      "comment_text    153164 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "(312735,)\n"
     ]
    }
   ],
   "source": [
    "print train_text.shape\n",
    "print test_text.shape\n",
    "print train.info()\n",
    "print test.info()\n",
    "print all_text.shape\n",
    "#print all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont use errors='ignore'\n",
    "#all_text.comment_text = map(lambda x:unicode(x, 'utf-8'), all_text.comment_text)\n",
    "all_text = map(lambda x:unicode(x, 'utf-8'), all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'i', u'me', u'my', u'myself', u'we', u'our', u'ours',\n",
       "       u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\",\n",
       "       u'your', u'yours', u'yourself', u'yourselves', u'he', u'him',\n",
       "       u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself',\n",
       "       u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their',\n",
       "       u'theirs', u'themselves', u'what', u'which', u'who', u'whom',\n",
       "       u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is',\n",
       "       u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has',\n",
       "       u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an',\n",
       "       u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until',\n",
       "       u'while', u'of', u'at', u'by', u'for', u'with', u'about',\n",
       "       u'against', u'between', u'into', u'through', u'during', u'before',\n",
       "       u'after', u'above', u'below', u'to', u'from', u'up', u'down',\n",
       "       u'in', u'out', u'on', u'off', u'over', u'under', u'again',\n",
       "       u'further', u'then', u'once', u'here', u'there', u'when', u'where',\n",
       "       u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more',\n",
       "       u'most', u'other', u'some', u'such', u'no', u'nor', u'not',\n",
       "       u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's',\n",
       "       u't', u'can', u'will', u'just', u'don', u\"don't\", u'should',\n",
       "       u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y',\n",
       "       u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn',\n",
       "       u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn',\n",
       "       u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma',\n",
       "       u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\",\n",
       "       u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\",\n",
       "       u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting the stopwords from nltk library\n",
    "sw = stopwords.words('english')\n",
    "# displaying the stopwords\n",
    "np.array(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(text):\n",
    "    '''a function for removing the stopword'''\n",
    "# removing the stop words and lowercasing the selected words\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    # joining the list of words with space separator\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####all_text['comment_text'] = all_text['comment_text'].apply(stopwords)\n",
    "#all_text.head(10)\n",
    "all_text = map(stopwords, all_text)\n",
    "#all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PunctuationToRemove = [\".\", \",\", \":\", \";\", \"!\" ,\"?\", \"&\", \"\\\"\", \"\\'\", \"~\", \"\\\\\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    '''a function for removing punctuation'''\n",
    "    for char in PunctuationToRemove:\n",
    "        text = text.replace(char,\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_text['comment_text'] = all_text['comment_text'].apply(remove_punctuation)\n",
    "#all_text.head(10)\n",
    "all_text = map(remove_punctuation, all_text)\n",
    "#all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#separate best words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_train = train.loc[train.toxic==1, ]\n",
    "sev_train = train.loc[train.severe_toxic==1, ]\n",
    "obs_train = train.loc[train.obscene==1, ]\n",
    "threat_train = train.loc[train.threat==1, ]\n",
    "ins_train = train.loc[train.insult==1, ]\n",
    "ident_train = train.loc[train.identity_hate==1, ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_text = map(lambda x:unicode(x, 'utf-8'), tox_train.comment_text)\n",
    "tox_text = map(stopwords, tox_text)\n",
    "tox_text = map(remove_punctuation, tox_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14867 tokens in Comment_text if we use word\n"
     ]
    }
   ],
   "source": [
    "#for toxic\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    encoding='utf-8',\n",
    "    lowercase=True,\n",
    "    min_df=0.0001,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    #use_idf=1, smooth_idf=1,\n",
    "    max_features=None)\n",
    "\n",
    "word_vectorizer.fit(tox_text)\n",
    "tox_word_features = word_vectorizer.transform(tox_text)\n",
    "\n",
    "msg = \"There are {} tokens in Comment_text if we use word\"\n",
    "print(msg.format(len(word_vectorizer.get_feature_names())))\n",
    "tox_words = word_vectorizer.get_feature_names()[541:]\n",
    "#tox_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sev_text = map(lambda x:unicode(x, 'utf-8'), sev_train.comment_text)\n",
    "sev_text = map(stopwords, sev_text)\n",
    "sev_text = map(remove_punctuation, sev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1993 tokens in Comment_text if we use word\n"
     ]
    }
   ],
   "source": [
    "#for sev_toxic\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    encoding='utf-8',\n",
    "    lowercase=True,\n",
    "    min_df=0.001,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    #use_idf=1, smooth_idf=1,\n",
    "    max_features=None)\n",
    "\n",
    "word_vectorizer.fit(sev_text)\n",
    "sev_word_features = word_vectorizer.transform(sev_text)\n",
    "\n",
    "msg = \"There are {} tokens in Comment_text if we use word\"\n",
    "print(msg.format(len(word_vectorizer.get_feature_names())))\n",
    "sev_words = word_vectorizer.get_feature_names()\n",
    "#sev_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_text = map(lambda x:unicode(x, 'utf-8'), obs_train.comment_text)\n",
    "obs_text = map(stopwords, obs_text)\n",
    "obs_text = map(remove_punctuation, obs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2159 tokens in Comment_text if we use word\n"
     ]
    }
   ],
   "source": [
    "#for obs\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    encoding='utf-8',\n",
    "    lowercase=True,\n",
    "    min_df=0.001,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    #use_idf=1, smooth_idf=1,\n",
    "    max_features=None)\n",
    "\n",
    "word_vectorizer.fit(obs_text)\n",
    "obs_word_features = word_vectorizer.transform(obs_text)\n",
    "\n",
    "msg = \"There are {} tokens in Comment_text if we use word\"\n",
    "print(msg.format(len(word_vectorizer.get_feature_names())))\n",
    "obs_words = word_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_text = map(lambda x:unicode(x, 'utf-8'), threat_train.comment_text)\n",
    "threat_text = map(stopwords, threat_text)\n",
    "threat_text = map(remove_punctuation, threat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2838 tokens in Comment_text if we use word\n"
     ]
    }
   ],
   "source": [
    "#for threat\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    encoding='utf-8',\n",
    "    lowercase=True,\n",
    "    min_df=0.0001,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    #use_idf=1, smooth_idf=1,\n",
    "    max_features=None)\n",
    "\n",
    "word_vectorizer.fit(threat_text)\n",
    "threat_word_features = word_vectorizer.transform(threat_text)\n",
    "\n",
    "msg = \"There are {} tokens in Comment_text if we use word\"\n",
    "print(msg.format(len(word_vectorizer.get_feature_names())))\n",
    "threat_words = word_vectorizer.get_feature_names()[69:]\n",
    "#threat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_text = map(lambda x:unicode(x, 'utf-8'), ins_train.comment_text)\n",
    "ins_text = map(stopwords, ins_text)\n",
    "ins_text = map(remove_punctuation, ins_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5623 tokens in Comment_text if we use word\n"
     ]
    }
   ],
   "source": [
    "#for ins\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    encoding='utf-8',\n",
    "    lowercase=True,\n",
    "    min_df=0.0003,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    #use_idf=1, smooth_idf=1,\n",
    "    max_features=None)\n",
    "\n",
    "word_vectorizer.fit(ins_text)\n",
    "#tox_word_features = word_vectorizer.transform(tox_train)\n",
    "\n",
    "msg = \"There are {} tokens in Comment_text if we use word\"\n",
    "print(msg.format(len(word_vectorizer.get_feature_names())))\n",
    "ins_words = word_vectorizer.get_feature_names()\n",
    "#ins_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ins_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ident_text = map(lambda x:unicode(x, 'utf-8'), ident_train.comment_text)\n",
    "ident_text = map(stopwords, ident_text)\n",
    "ident_text = map(remove_punctuation, ident_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7437 tokens in Comment_text if we use word\n"
     ]
    }
   ],
   "source": [
    "#for identity\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    encoding='utf-8',\n",
    "    lowercase=True,\n",
    "    min_df=0.0001,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    #use_idf=1, smooth_idf=1,\n",
    "    max_features=None)\n",
    "\n",
    "word_vectorizer.fit(ident_text)\n",
    "#tox_word_features = word_vectorizer.transform(tox_train)\n",
    "\n",
    "msg = \"There are {} tokens in Comment_text if we use word\"\n",
    "print(msg.format(len(word_vectorizer.get_feature_names())))\n",
    "ident_words = word_vectorizer.get_feature_names()\n",
    "#ident_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ident_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = map(lambda x:unicode(x, 'utf-8'), test_text)\n",
    "test_text = map(stopwords, test_text)\n",
    "test_text = map(remove_punctuation, test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7870 tokens in Comment_text if we use word\n"
     ]
    }
   ],
   "source": [
    "#for test\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    encoding='utf-8',\n",
    "    lowercase=True,\n",
    "    min_df=0.0003,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    #use_idf=1, smooth_idf=1,\n",
    "    max_features=None)\n",
    "\n",
    "word_vectorizer.fit(test_text)\n",
    "#tox_word_features = word_vectorizer.transform(tox_train)\n",
    "\n",
    "msg = \"There are {} tokens in Comment_text if we use word\"\n",
    "print(msg.format(len(word_vectorizer.get_feature_names())))\n",
    "test_words = word_vectorizer.get_feature_names()\n",
    "#test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17979"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_words=[]\n",
    "for  word in tox_words:\n",
    "    imp_words.append(word)\n",
    "    \n",
    "for  word in sev_words:\n",
    "    imp_words.append(word)\n",
    "    \n",
    "for  word in obs_words:\n",
    "    imp_words.append(word)\n",
    "\n",
    "for  word in threat_words:\n",
    "    imp_words.append(word)\n",
    "    \n",
    "for  word in ins_words:\n",
    "    imp_words.append(word)\n",
    "    \n",
    "for  word in ident_words:\n",
    "    imp_words.append(word)\n",
    "\n",
    "for  word in test_words:\n",
    "    imp_words.append(word)\n",
    "\n",
    "imp_words_set=set(imp_words)\n",
    "imp_words = list(imp_words_set)\n",
    "len(imp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impwords(text):\n",
    "    '''a function for selecting important words'''\n",
    "\n",
    "    text = [word for word in text.split() if word in imp_words]\n",
    "        # joining the list of words with space separator\n",
    "    return \" \".join(text)\n",
    " #   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = map(impwords, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp_text = pd.DataFrame.from_dict({'comment': all_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp_text.to_csv('imptext.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17765 tokens in Comment_text if we use word\n"
     ]
    }
   ],
   "source": [
    "#whole data\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    encoding='utf-8',\n",
    "    lowercase=True,\n",
    "    #min_df=0.00001,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    #use_idf=1, smooth_idf=1,\n",
    "    max_features=None)\n",
    "\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "msg = \"There are {} tokens in Comment_text if we use word\"\n",
    "print(msg.format(len(word_vectorizer.get_feature_names())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.969070410186\n",
      "CV score for class severe_toxic is 0.98554066953\n",
      "CV score for class obscene is 0.98544702436\n",
      "CV score for class threat is 0.978086534108\n",
      "CV score for class insult is 0.976024437433\n",
      "CV score for class identity_hate is 0.971386806749\n",
      "Total CV score is 0.977592647061\n"
     ]
    }
   ],
   "source": [
    "#train_features = hstack([train_char_features, train_word_features])\n",
    "#test_features = hstack([test_char_features, test_word_features])\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "scores = []\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_word_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(train_word_features, train_target)\n",
    "    #submission[class_name] = classifier.predict_proba(test_word_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "#submission.to_csv('F:\\submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.958367413029\n",
      "CV score for class severe_toxic is 0.980475134741\n",
      "CV score for class obscene is 0.980033299749\n",
      "CV score for class threat is 0.969080010509\n",
      "CV score for class insult is 0.968672055661\n",
      "CV score for class identity_hate is 0.962001265406\n",
      "Total CV score is 0.969771529849\n"
     ]
    }
   ],
   "source": [
    "#train_features = hstack([train_char_features, train_word_features])\n",
    "#test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "\n",
    "scores = []\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_word_features, train_target, test_size=0.3, random_state=42)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, X_valid, y_valid, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "    #submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "#submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
